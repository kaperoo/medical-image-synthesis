{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/ed19e3c/env/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/users/ed19e3c/env/lib64/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from math import log2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pylab as plt\n",
    "from timm.models.layers import to_2tuple, trunc_normal_,  DropPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"../../utils/augmented_data/augmented_data/\"\n",
    "START_TRAIN_IMG_SIZE = 4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LR = 1e-3\n",
    "BATCH_SIZES = [16,16,16,8,8,8,8,4]\n",
    "CHANNELS_IMG = 1\n",
    "Z_DIm = 512\n",
    "W_DIM = 512\n",
    "IN_CHANNELS = 512\n",
    "LAMBDA_GP = 10\n",
    "PROGRESSIVE_EPOCHS = [20] * len(BATCH_SIZES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(image_size, S=0):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Resize((image_size, image_size)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.RandomHorizontalFlip(p=0.5),\n",
    "         transforms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)],\n",
    "            [0.5 for _ in range(CHANNELS_IMG)],\n",
    "         )\n",
    "        ]\n",
    "    )\n",
    "    if S==0:\n",
    "        batch_size = BATCH_SIZES[int(log2(image_size/4))]\n",
    "    else:\n",
    "        batch_size = S\n",
    "    dataset = datasets.ImageFolder(root=OUTPUT_PATH, transform=transform)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_loader():\n",
    "    loader,_ = get_loader(128)\n",
    "    cloth,_  = next(iter(loader))\n",
    "    _,ax     = plt.subplots(3,3,figsize=(8,8))\n",
    "    plt.suptitle('Some real samples')\n",
    "    ind = 0\n",
    "    for k in range(3):\n",
    "        for kk in range(3):\n",
    "            ax[k][kk].imshow((cloth[ind].permute(1,2,0)+1)/2)\n",
    "            ind +=1\n",
    "#check_loader() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noice apping network\n",
    "# AdaIN\n",
    "# Progressice growing\n",
    "factors = [1,1,1,1/2,1/4,1/8,1/16,1/32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_features, out_features\n",
    "    ):\n",
    "        super(WSLinear,self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.scale  = (2/in_features) ** 0.5\n",
    "        self.bias   = self.linear.bias\n",
    "        self.linear.bias = None\n",
    "\n",
    "        nn.init.normal_(self.linear.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        #print(self.linear)\n",
    "        #print(self.scale)\n",
    "        return self.linear(x * self.scale) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None):\n",
    "        super().__init__()\n",
    "        act_layer = nn.GELU\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(0.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "        pretrained_window_size (tuple[int]): The height and width of the window in pre-training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.,\n",
    "                 pretrained_window_size=[0, 0]):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.pretrained_window_size = pretrained_window_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)\n",
    "\n",
    "        # mlp to generate continuous relative position bias\n",
    "        self.cpb_mlp = nn.Sequential(nn.Linear(2, 512, bias=True),\n",
    "                                     nn.ReLU(inplace=True),\n",
    "                                     nn.Linear(512, num_heads, bias=False))\n",
    "\n",
    "        # get relative_coords_table\n",
    "        relative_coords_h = torch.arange(-(self.window_size[0] - 1), self.window_size[0], dtype=torch.float32)\n",
    "        relative_coords_w = torch.arange(-(self.window_size[1] - 1), self.window_size[1], dtype=torch.float32)\n",
    "        relative_coords_table = torch.stack(\n",
    "            torch.meshgrid([relative_coords_h,\n",
    "                            relative_coords_w])).permute(1, 2, 0).contiguous().unsqueeze(0)  # 1, 2*Wh-1, 2*Ww-1, 2\n",
    "        if pretrained_window_size[0] > 0:\n",
    "            relative_coords_table[:, :, :, 0] /= (pretrained_window_size[0] - 1)\n",
    "            relative_coords_table[:, :, :, 1] /= (pretrained_window_size[1] - 1)\n",
    "        else:\n",
    "            relative_coords_table[:, :, :, 0] /= (self.window_size[0] - 1)\n",
    "            relative_coords_table[:, :, :, 1] /= (self.window_size[1] - 1)\n",
    "        relative_coords_table *= 8  # normalize to -8, 8\n",
    "        relative_coords_table = torch.sign(relative_coords_table) * torch.log2(\n",
    "            torch.abs(relative_coords_table) + 1.0) / np.log2(8)\n",
    "\n",
    "        self.register_buffer(\"relative_coords_table\", relative_coords_table)\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        if qkv_bias:\n",
    "            self.q_bias = nn.Parameter(torch.zeros(dim))\n",
    "            self.v_bias = nn.Parameter(torch.zeros(dim))\n",
    "        else:\n",
    "            self.q_bias = None\n",
    "            self.v_bias = None\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv_bias = None\n",
    "        if self.q_bias is not None:\n",
    "            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n",
    "        \n",
    "        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n",
    "        qkv = qkv.reshape(B_, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        # cosine attention\n",
    "        attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))\n",
    "        logit_scale = torch.clamp(self.logit_scale, max=torch.log(torch.tensor(1. / 0.01).to(DEVICE))).exp()\n",
    "        attn = attn * logit_scale\n",
    "\n",
    "        relative_position_bias_table = self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads)\n",
    "        relative_position_bias = relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        relative_position_bias = 16 * torch.sigmoid(relative_position_bias)\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, ' \\\n",
    "               f'pretrained_window_size={self.pretrained_window_size}, num_heads={self.num_heads}'\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 window with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixenNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixenNorm, self).__init__()\n",
    "        self.epsilon = 1e-8\n",
    "    def forward(self,x ):\n",
    "        return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True)+  self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaIN(nn.Module):\n",
    "    def __init__(self, channels, w_dim):\n",
    "        super().__init__()\n",
    "        self.instance_norm = nn.InstanceNorm2d(channels)\n",
    "        self.style_scale   = WSLinear(w_dim, channels)\n",
    "        self.style_bias    = WSLinear(w_dim, channels)\n",
    "\n",
    "    def forward(self,x,w):\n",
    "        x = self.instance_norm(x)\n",
    "        #print(w.shape)\n",
    "        style_scale = self.style_scale(w).unsqueeze(2).unsqueeze(3)\n",
    "        style_bias  = self.style_bias(w).unsqueeze(2).unsqueeze(3)\n",
    "        return style_scale * x + style_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class injectNoise(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(1,channels,1,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        noise = torch.randn((x.shape[0], 1, x.shape[2], x.shape[3]), device = x.device)\n",
    "        return x + self.weight + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(2 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.reduction(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
    "        flops += H * W * self.dim // 2\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "        pretrained_window_size (int): Local window size in pre-training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n",
    "                 pretrained_window_size=0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer,\n",
    "                                 pretrained_window_size=pretrained_window_size)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        for blk in self.blocks:\n",
    "            flops += blk.flops()\n",
    "        if self.downsample is not None:\n",
    "            flops += self.downsample.flops()\n",
    "        return flops\n",
    "\n",
    "    def _init_respostnorm(self):\n",
    "        for blk in self.blocks:\n",
    "            nn.init.constant_(blk.norm1.bias, 0)\n",
    "            nn.init.constant_(blk.norm1.weight, 0)\n",
    "            nn.init.constant_(blk.norm2.bias, 0)\n",
    "            nn.init.constant_(blk.norm2.weight, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        Ho, Wo = self.patches_resolution\n",
    "        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n",
    "        if self.norm is not None:\n",
    "            flops += Ho * Wo * self.embed_dim\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "        pretrained_window_size (int): Window size in pre-training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=4, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, pretrained_window_size=0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop,\n",
    "            pretrained_window_size=to_2tuple(pretrained_window_size))\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "    \n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "        x = shortcut + self.drop_path(self.norm1(x))\n",
    "\n",
    "        # FFN\n",
    "        x = x + self.drop_path(self.norm2(self.mlp(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.input_resolution\n",
    "        # norm1\n",
    "        flops += self.dim * H * W\n",
    "        # W-MSA/SW-MSA\n",
    "        nW = H * W / self.window_size / self.window_size\n",
    "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
    "        # mlp\n",
    "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
    "        # norm2\n",
    "        flops += self.dim * H * W\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SwinTransformerV2(nn.Module):\n",
    "#     r\"\"\" Swin Transformer\n",
    "#         A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "#           https://arxiv.org/pdf/2103.14030\n",
    "\n",
    "#     Args:\n",
    "#         img_size (int | tuple(int)): Input image size. Default 224\n",
    "#         patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "#         in_chans (int): Number of input image channels. Default: 3\n",
    "#         num_classes (int): Number of classes for classification head. Default: 1000\n",
    "#         embed_dim (int): Patch embedding dimension. Default: 96\n",
    "#         depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "#         num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "#         window_size (int): Window size. Default: 7\n",
    "#         mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "#         qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "#         drop_rate (float): Dropout rate. Default: 0\n",
    "#         attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "#         drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "#         norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "#         ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "#         patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "#         use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "#         pretrained_window_sizes (tuple(int)): Pretrained window sizes of each layer.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, img_size=512, patch_size=4, in_chans=1, num_classes=512,\n",
    "#                  embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
    "#                  window_size=4, mlp_ratio=4., qkv_bias=True,\n",
    "#                  drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "#                  norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "#                  use_checkpoint=False, pretrained_window_sizes=[0, 0, 0, 0], **kwargs):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.num_classes = num_classes\n",
    "#         self.num_layers = len(depths)\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.ape = ape\n",
    "#         self.patch_norm = patch_norm\n",
    "#         self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "#         self.mlp_ratio = mlp_ratio\n",
    "\n",
    "#         # split image into non-overlapping patches\n",
    "#         self.patch_embed = PatchEmbed(\n",
    "#             img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "#             norm_layer=norm_layer if self.patch_norm else None)\n",
    "#         num_patches = self.patch_embed.num_patches\n",
    "#         patches_resolution = self.patch_embed.patches_resolution\n",
    "#         self.patches_resolution = patches_resolution\n",
    "\n",
    "#         # absolute position embedding\n",
    "#         if self.ape:\n",
    "#             self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "#             trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "#         self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "#         # stochastic depth\n",
    "#         dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "#         # build layers\n",
    "#         self.layers = nn.ModuleList()\n",
    "#         for i_layer in range(self.num_layers):\n",
    "#             layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "#                                input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "#                                                  patches_resolution[1] // (2 ** i_layer)),\n",
    "#                                depth=depths[i_layer],\n",
    "#                                num_heads=num_heads[i_layer],\n",
    "#                                window_size=window_size,\n",
    "#                                mlp_ratio=self.mlp_ratio,\n",
    "#                                qkv_bias=qkv_bias,\n",
    "#                                drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "#                                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "#                                norm_layer=norm_layer,\n",
    "#                                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "#                                use_checkpoint=use_checkpoint,\n",
    "#                                pretrained_window_size=pretrained_window_sizes[i_layer])\n",
    "#             self.layers.append(layer)\n",
    "\n",
    "#         self.norm = norm_layer(self.num_features)\n",
    "#         self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "#         self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "#         self.apply(self._init_weights)\n",
    "#         for bly in self.layers:\n",
    "#             bly._init_respostnorm()\n",
    "\n",
    "#     def _init_weights(self, m):\n",
    "#         if isinstance(m, nn.Linear):\n",
    "#             trunc_normal_(m.weight, std=.02)\n",
    "#             if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "#         elif isinstance(m, nn.LayerNorm):\n",
    "#             nn.init.constant_(m.bias, 0)\n",
    "#             nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "#     @torch.jit.ignore\n",
    "#     def no_weight_decay(self):\n",
    "#         return {'absolute_pos_embed'}\n",
    "\n",
    "#     @torch.jit.ignore\n",
    "#     def no_weight_decay_keywords(self):\n",
    "#         return {\"cpb_mlp\", \"logit_scale\", 'relative_position_bias_table'}\n",
    "\n",
    "#     def forward_features(self, x):\n",
    "#         x = self.patch_embed(x)\n",
    "#         if self.ape:\n",
    "#             x = x + self.absolute_pos_embed\n",
    "#         x = self.pos_drop(x)\n",
    "\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x)\n",
    "\n",
    "#         x = self.norm(x)  # B L C\n",
    "#         #print(x.shape)\n",
    "#         x = self.avgpool(x.transpose(1, 2))  # B C 1\n",
    "#         #print(x.shape)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.forward_features(x)\n",
    "#         x = self.head(x)\n",
    "#         #print(self.num_features)\n",
    "#         #print(\"head: \"+ str(x.shape))\n",
    "#         return x\n",
    "\n",
    "#     def flops(self):\n",
    "#         flops = 0\n",
    "#         flops += self.patch_embed.flops()\n",
    "#         for i, layer in enumerate(self.layers):\n",
    "#             flops += layer.flops()\n",
    "#         flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n",
    "#         flops += self.num_features * self.num_classes\n",
    "#         return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerV2(nn.Module):\n",
    "    r\"\"\" Swin Transformer\n",
    "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 224\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "        pretrained_window_sizes (tuple(int)): Pretrained window sizes of each layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=512, patch_size=4, in_chans=1, num_classes=512,\n",
    "                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
    "                 window_size=4, mlp_ratio=4., qkv_bias=True,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_checkpoint=False, pretrained_window_sizes=[0, 0, 0, 0], **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size,\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                               use_checkpoint=use_checkpoint,\n",
    "                               pretrained_window_size=pretrained_window_sizes[i_layer])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for bly in self.layers:\n",
    "            bly._init_respostnorm()\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {\"cpb_mlp\", \"logit_scale\", 'relative_position_bias_table'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "        #print(x.shape)\n",
    "        x = self.avgpool(x.transpose(1, 2))  # B C 1\n",
    "        #print(x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        #print(self.num_features)\n",
    "        #print(\"head: \"+ str(x.shape))\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n",
    "        flops += self.num_features * self.num_classes\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, w_dim):\n",
    "        super(GenBlock, self).__init__()\n",
    "        self.conv1 = WSConv2d(in_channel, out_channel)\n",
    "        self.conv2 = WSConv2d(out_channel, out_channel)\n",
    "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.inject_noise1 = injectNoise(out_channel)\n",
    "        self.inject_noise2 = injectNoise(out_channel)\n",
    "        self.adain1 = AdaIN(out_channel, w_dim)\n",
    "        self.adain2 = AdaIN(out_channel, w_dim)\n",
    "    def forward(self, x,w):\n",
    "        x = self.adain1(self.leaky(self.inject_noise1(self.conv1(x))), w)\n",
    "        x = self.adain2(self.leaky(self.inject_noise2(self.conv2(x))), w)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, w_dim, in_channels, img_channels=1, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.starting_cte = nn.Parameter(torch.ones(1, in_channels, 4,4))\n",
    "        self.style_swin_encoder = SwinTransformerV2()\n",
    "        #self.map = MappingNetwork(z_dim+num_classes, w_dim)\n",
    "        self.initial_adain1 = AdaIN(in_channels, w_dim)\n",
    "        self.initial_adain2 = AdaIN(in_channels, w_dim)\n",
    "        self.initial_noise1 = injectNoise(in_channels)\n",
    "        self.initial_noise2 = injectNoise(in_channels)\n",
    "        self.initial_conv   = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.leaky          = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "        self.initial_rgb    = WSConv2d(\n",
    "            in_channels, img_channels, kernel_size = 1, stride=1, padding=0\n",
    "        )\n",
    "        self.prog_blocks, self.rgb_layers = (\n",
    "            nn.ModuleList([]),\n",
    "            nn.ModuleList([self.initial_rgb])\n",
    "        )\n",
    "\n",
    "        for i in range(len(factors)-1):\n",
    "            conv_in_c  = int(in_channels * factors[i])\n",
    "            conv_out_c = int(in_channels * factors[i+1])\n",
    "            self.prog_blocks.append(GenBlock(conv_in_c, conv_out_c, w_dim))\n",
    "            self.rgb_layers.append(WSConv2d(conv_out_c, img_channels, kernel_size = 1, stride=1, padding=0))\n",
    "        \n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        return torch.tanh(alpha * generated + (1-alpha ) * upscaled)\n",
    "\n",
    "    def forward(self, noise, real, alpha, steps, label):\n",
    "        label_onehot = F.one_hot(label, num_classes=7).float()\n",
    "        noise = torch.cat((noise, label_onehot), dim=1)\n",
    "        \n",
    "        #w = self.map(noise)\n",
    "        w = self.style_swin_encoder(real).to(DEVICE)\n",
    "        #print(w.shape)\n",
    "        x = self.initial_adain1(self.initial_noise1(self.starting_cte),w)\n",
    "        x = self.initial_conv(x)\n",
    "        out = self.initial_adain2(self.leaky(self.initial_noise2(x)), w)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(x)\n",
    "        \n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode = 'bilinear')\n",
    "            out      = self.prog_blocks[step](upscaled,w)\n",
    "\n",
    "        final_upscaled = self.rgb_layers[steps-1](upscaled)\n",
    "        final_out      = self.rgb_layers[steps](out)\n",
    "\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSConv2d(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "    ):\n",
    "        super(WSConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (2 / (in_channels * (kernel_size ** 2))) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        # initialize conv layer\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels, img_channels=1, num_classes=7):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding = nn.Embedding(num_classes, in_channels)\n",
    "\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        # here we work back ways from factors because the discriminator\n",
    "        # should be mirrored from the generator. So the first prog_block and\n",
    "        # rgb layer we append will work for input size 1024x1024, then 512->256-> etc\n",
    "        for i in range(len(factors) - 1, 0, -1):\n",
    "            conv_in = int(in_channels * factors[i])\n",
    "            conv_out = int(in_channels * factors[i - 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in, conv_out))\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "        # perhaps confusing name \"initial_rgb\" this is just the RGB layer for 4x4 input size\n",
    "        # did this to \"mirror\" the generator initial_rgb\n",
    "        self.initial_rgb = WSConv2d(\n",
    "            img_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(\n",
    "            kernel_size=2, stride=2\n",
    "        )  # down sampling using avg pool\n",
    "\n",
    "        # this is the block for 4x4 input size\n",
    "        self.final_block = nn.Sequential(\n",
    "            # +1 to in_channels because we concatenate from MiniBatch std\n",
    "            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(\n",
    "                in_channels, 1, kernel_size=1, padding=0, stride=1\n",
    "            ),  # we use this instead of linear layer\n",
    "        )\n",
    "\n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "        \"\"\"Used to fade in downscaled using avg pooling and output from CNN\"\"\"\n",
    "        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "\n",
    "    def minibatch_std(self, x):\n",
    "        batch_statistics = (\n",
    "            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        )\n",
    "        # we take the std for each example (across all channels, and pixels) then we repeat it\n",
    "        # for a single channel and concatenate it with the image. In this way the discriminator\n",
    "        # will get information about the variation in the batch/image\n",
    "        return torch.cat([x, batch_statistics], dim=1)\n",
    "\n",
    "    def forward(self, x, alpha, steps, labels):\n",
    "        # where we should start in the list of prog_blocks, maybe a bit confusing but\n",
    "        # the last is for the 4x4. So example let's say steps=1, then we should start\n",
    "        # at the second to last because input_size will be 8x8. If steps==0 we just\n",
    "        # use the final block\n",
    "        cur_step = len(self.prog_blocks) - steps\n",
    "\n",
    "        # convert from rgb as initial step, this will depend on\n",
    "        # the image size (each will have it's on rgb layer)\n",
    "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
    "\n",
    "        if steps == 0:  # i.e, image is 4x4\n",
    "            out = self.minibatch_std(out)\n",
    "            out = self.final_block(out).view(out.shape[0], -1)\n",
    "\n",
    "            label_embedding = self.embedding(labels)\n",
    "            out = torch.sum(out*label_embedding, dim=1, keepdim=True)\n",
    "            return out \n",
    "\n",
    "        # because prog_blocks might change the channels, for down scale we use rgb_layer\n",
    "        # from previous/smaller size which in our case correlates to +1 in the indexing\n",
    "        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "\n",
    "        # the fade_in is done first between the downscaled and the input\n",
    "        # this is opposite from the generator\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        for step in range(cur_step + 1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_std(out)\n",
    "        out = self.final_block(out).view(out.shape[0], -1)\n",
    "\n",
    "        label_embedding = self.embedding(labels)\n",
    "        out = torch.sum(out * label_embedding, dim=1, keepdim=True)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(gen, steps, loader=None, n=100):\n",
    "\n",
    "    gen.eval()\n",
    "    alpha = 1.0\n",
    "    for i in range(n):\n",
    "        with torch.no_grad():\n",
    "            noise = torch.randn(1, Z_DIm).to(DEVICE)\n",
    "            label = torch.randint(0, 6, (1, )).to(DEVICE)\n",
    "            for image, _ in loader:\n",
    "                idx = random.randint(0, image.size(0) - 1)\n",
    "                random_image = image[idx]\n",
    "                break\n",
    "            random_image = random_image.unsqueeze(1)\n",
    "            random_image = random_image.to(DEVICE)\n",
    "            img = gen(noise, random_image, alpha, steps, label)\n",
    "            if not os.path.exists(f'saved_examples/step{steps}'):\n",
    "                os.makedirs(f'saved_examples/step{steps}')\n",
    "            save_image(img*0.5+0.5, f\"saved_examples/step{steps}/img_{i}.png\")\n",
    "    gen.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake, alpha, train_step, label, device=\"cuda\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images, alpha, train_step, label)\n",
    " \n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    critic,\n",
    "    gen,\n",
    "    loader,\n",
    "    dataset,\n",
    "    large_loader,\n",
    "    large_dataset,\n",
    "    step,\n",
    "    alpha,\n",
    "    opt_critic,\n",
    "    opt_gen\n",
    "):\n",
    "    loop = tqdm(zip(loader, large_loader), leave=True)\n",
    "\n",
    "    for batch_idx, ((real, label), (large_real, _)) in enumerate(loop):\n",
    "        real = real.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        large_real = large_real.to(DEVICE)\n",
    "        cur_batch_size = real.shape[0]\n",
    "        noise = torch.randn(cur_batch_size, Z_DIm).to(DEVICE)\n",
    "        fake  = gen(noise, large_real, alpha, step, label)\n",
    "        critic_real = critic(real, alpha, step, label)\n",
    "        critic_fake = critic(fake.detach(), alpha, step, label)\n",
    "        gp = gradient_penalty(critic, real, fake, alpha, step, label, DEVICE)\n",
    "        loss_critic = (\n",
    "            -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
    "            + LAMBDA_GP * gp\n",
    "            + (0.001) * torch.mean(critic_real ** 2)\n",
    "        )\n",
    "\n",
    "        critic.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        opt_critic.step()\n",
    "\n",
    "        gen_fake = critic(fake, alpha, step, label)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        alpha += cur_batch_size / (\n",
    "            PROGRESSIVE_EPOCHS[step] * 0.5 * len(dataset)\n",
    "        )\n",
    "        alpha = min(alpha,1)\n",
    "\n",
    "\n",
    "        loop.set_postfix(\n",
    "            gp = gp.item(),\n",
    "            loss_critic = loss_critic.item()\n",
    "        )\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uolstore/home/users/ed19e3c/.local/lib/python3.9/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent image size: 4\n",
      "Epoch [1/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:16,  1.69it/s, gp=0.107, loss_critic=-0.228] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:05,  1.97it/s, gp=0.0855, loss_critic=0.65]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:05,  1.98it/s, gp=0.0736, loss_critic=0.295]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:05,  1.97it/s, gp=747, loss_critic=-2.5e+5]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:05,  1.98it/s, gp=115, loss_critic=1.03e+3]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:05,  1.96it/s, gp=4.08, loss_critic=3.17] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:04,  1.99it/s, gp=10.6, loss_critic=-71.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:05,  1.96it/s, gp=1.44, loss_critic=-11.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:05,  1.96it/s, gp=4.52, loss_critic=-31.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:05,  1.97it/s, gp=1.55, loss_critic=-27.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:06,  1.95it/s, gp=0.797, loss_critic=-14.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:06,  1.95it/s, gp=353, loss_critic=3.03e+3]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:05,  1.97it/s, gp=35.7, loss_critic=233]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:06,  1.95it/s, gp=11.7, loss_critic=51.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:06,  1.95it/s, gp=10, loss_critic=2.97]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:05,  1.96it/s, gp=5.36, loss_critic=-39.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:05,  1.98it/s, gp=2.4, loss_critic=-18.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:05,  1.97it/s, gp=2.19, loss_critic=-22.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:05,  1.97it/s, gp=0.52, loss_critic=-14.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:05,  1.97it/s, gp=1.7e+3, loss_critic=1.49e+4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent image size: 8\n",
      "Epoch [1/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:07,  1.90it/s, gp=2.6, loss_critic=28.9]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:07,  1.91it/s, gp=0.41, loss_critic=3.82] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:07,  1.91it/s, gp=0.919, loss_critic=9.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:07,  1.91it/s, gp=0.398, loss_critic=3.43]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:07,  1.92it/s, gp=0.239, loss_critic=0.393]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:07,  1.90it/s, gp=0.382, loss_critic=-1.28]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:07,  1.92it/s, gp=0.251, loss_critic=0.0264] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:07,  1.91it/s, gp=0.0223, loss_critic=-1.26]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:06,  1.95it/s, gp=0.0619, loss_critic=-2.35] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:07,  1.90it/s, gp=0.0184, loss_critic=-3.83] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:07,  1.92it/s, gp=0.0036, loss_critic=-3.33] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:07,  1.92it/s, gp=0.00256, loss_critic=-3.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:06,  1.94it/s, gp=0.151, loss_critic=-2.82] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:07,  1.91it/s, gp=0.0228, loss_critic=-2.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:07,  1.91it/s, gp=0.0281, loss_critic=-1.59]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:07,  1.91it/s, gp=0.0557, loss_critic=-2.43] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:07,  1.92it/s, gp=0.0443, loss_critic=-2.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:07,  1.91it/s, gp=0.101, loss_critic=-3.37] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:07,  1.90it/s, gp=0.00778, loss_critic=-4.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:07,  1.92it/s, gp=0.0329, loss_critic=-4.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent image size: 16\n",
      "Epoch [1/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:12,  1.78it/s, gp=0.0986, loss_critic=-7.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:11,  1.80it/s, gp=0.0468, loss_critic=-8.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:12,  1.79it/s, gp=0.174, loss_critic=-11.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:12,  1.79it/s, gp=0.12, loss_critic=-8.4]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:12,  1.79it/s, gp=0.573, loss_critic=-3.59]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:12,  1.78it/s, gp=0.0842, loss_critic=-10.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:13,  1.76it/s, gp=0.333, loss_critic=-10.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:13,  1.75it/s, gp=0.252, loss_critic=-12.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:12,  1.77it/s, gp=0.183, loss_critic=-9.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:12,  1.78it/s, gp=0.00414, loss_critic=-2.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:11,  1.79it/s, gp=0.249, loss_critic=-6.53] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:12,  1.79it/s, gp=0.242, loss_critic=-9.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:11,  1.80it/s, gp=0.334, loss_critic=-11.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:12,  1.78it/s, gp=0.137, loss_critic=-11.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:11,  1.79it/s, gp=0.0412, loss_critic=-9.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:12,  1.79it/s, gp=0.311, loss_critic=-10.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:11,  1.79it/s, gp=0.275, loss_critic=-9.18] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:11,  1.79it/s, gp=0.00508, loss_critic=-7.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:11,  1.80it/s, gp=0.0911, loss_critic=-8.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [01:12,  1.79it/s, gp=0.588, loss_critic=-9.43] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent image size: 32\n",
      "Epoch [1/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:27,  2.94it/s, gp=0.628, loss_critic=-23.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:27,  2.95it/s, gp=0.635, loss_critic=-26.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:27,  2.95it/s, gp=1.45, loss_critic=-23.9]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:26,  2.96it/s, gp=0.842, loss_critic=-25.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:27,  2.95it/s, gp=2.1, loss_critic=-20.2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:26,  2.97it/s, gp=8.13, loss_critic=-106]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:26,  2.96it/s, gp=1.19, loss_critic=-64.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:27,  2.95it/s, gp=22, loss_critic=-35.4]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:27,  2.95it/s, gp=5.29, loss_critic=-157]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:27,  2.95it/s, gp=4.06, loss_critic=-131]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:26,  2.97it/s, gp=8.02, loss_critic=-91]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:26,  2.97it/s, gp=4.9, loss_critic=-136]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:27,  2.95it/s, gp=3.2, loss_critic=-120]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:26,  2.97it/s, gp=17.6, loss_critic=-59.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:27,  2.95it/s, gp=3.86, loss_critic=-128]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:26,  2.96it/s, gp=4.33, loss_critic=-126]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:26,  2.96it/s, gp=5.93, loss_critic=-127] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:26,  2.97it/s, gp=8.04, loss_critic=-122] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:26,  2.97it/s, gp=7.08, loss_critic=-105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:27,  2.95it/s, gp=2.13, loss_critic=-89.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent image size: 64\n",
      "Epoch [1/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:44,  2.45it/s, gp=14.7, loss_critic=-412] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:44,  2.45it/s, gp=35.7, loss_critic=-370] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:44,  2.46it/s, gp=18.4, loss_critic=-464] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:44,  2.46it/s, gp=2.91, loss_critic=-53.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:44,  2.45it/s, gp=3.64, loss_critic=-61.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:44,  2.45it/s, gp=3.66, loss_critic=-71.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:44,  2.45it/s, gp=1.8, loss_critic=-64.7]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:44,  2.46it/s, gp=2.87, loss_critic=-83.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:48,  2.37it/s, gp=2.86, loss_critic=-61.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:46,  2.41it/s, gp=5.81, loss_critic=-80.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:45,  2.43it/s, gp=3.84, loss_critic=-73.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:44,  2.45it/s, gp=4.28, loss_critic=-75.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:45,  2.45it/s, gp=4.59, loss_critic=-78.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:45,  2.45it/s, gp=1.78, loss_critic=-69.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:44,  2.46it/s, gp=3.1, loss_critic=-79.7]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:44,  2.46it/s, gp=1.84, loss_critic=-72.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:44,  2.45it/s, gp=3.87, loss_critic=-64.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:44,  2.45it/s, gp=2.18, loss_critic=-65.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:45,  2.44it/s, gp=2.11, loss_critic=-74.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [01:45,  2.44it/s, gp=4.82, loss_critic=-83.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent image size: 128\n",
      "Epoch [1/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:12,  1.95it/s, gp=12.7, loss_critic=-202]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:12,  1.94it/s, gp=22, loss_critic=-155]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:12,  1.94it/s, gp=14.1, loss_critic=-226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:12,  1.94it/s, gp=11.3, loss_critic=-206] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:11,  1.95it/s, gp=17.3, loss_critic=-247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:12,  1.94it/s, gp=14.2, loss_critic=-219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:12,  1.93it/s, gp=13.1, loss_critic=-243]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:12,  1.93it/s, gp=8.61, loss_critic=-176] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:12,  1.94it/s, gp=7.35, loss_critic=-133] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:12,  1.94it/s, gp=0.263, loss_critic=-15]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:12,  1.94it/s, gp=0.147, loss_critic=-10.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:12,  1.94it/s, gp=0.187, loss_critic=-11.9]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:12,  1.93it/s, gp=0.0262, loss_critic=-5.18] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:12,  1.94it/s, gp=0.241, loss_critic=-2.68]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:12,  1.93it/s, gp=0.259, loss_critic=5.7]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:12,  1.94it/s, gp=0.0638, loss_critic=0.843]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:12,  1.93it/s, gp=0.0341, loss_critic=-0.879] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:12,  1.94it/s, gp=0.0505, loss_critic=-2.51]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:14,  1.90it/s, gp=0.0143, loss_critic=-3.81]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [02:12,  1.94it/s, gp=0.957, loss_critic=-30.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent image size: 256\n",
      "Epoch [1/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:27,  1.24it/s, gp=0.0619, loss_critic=-9.55]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:27,  1.24it/s, gp=0.0183, loss_critic=-7.87]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:22,  1.27it/s, gp=0.136, loss_critic=-3.93]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:20,  1.28it/s, gp=0.142, loss_critic=-3.03]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:21,  1.28it/s, gp=0.0995, loss_critic=-2.57]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:20,  1.28it/s, gp=0.0466, loss_critic=-7.7]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:20,  1.28it/s, gp=0.123, loss_critic=-4.1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:20,  1.28it/s, gp=0.1, loss_critic=-0.575]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:21,  1.28it/s, gp=0.0118, loss_critic=-4.22]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:21,  1.28it/s, gp=0.392, loss_critic=-24.5]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:20,  1.28it/s, gp=0.14, loss_critic=-3.93]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:21,  1.28it/s, gp=0.0192, loss_critic=-4.72]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:20,  1.28it/s, gp=0.166, loss_critic=4.67]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:21,  1.28it/s, gp=0.0834, loss_critic=-3.21]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:21,  1.28it/s, gp=0.0242, loss_critic=3.93]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:20,  1.28it/s, gp=0.0209, loss_critic=-5.72]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:21,  1.27it/s, gp=0.401, loss_critic=-9.68]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:21,  1.28it/s, gp=0.102, loss_critic=-7]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:20,  1.28it/s, gp=0.0762, loss_critic=-13.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [03:20,  1.28it/s, gp=0.276, loss_critic=1.92]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent image size: 512\n",
      "Epoch [1/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "514it [09:45,  1.14s/it, gp=0.349, loss_critic=-22.4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "514it [09:44,  1.14s/it, gp=0.0443, loss_critic=-36.5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "514it [09:45,  1.14s/it, gp=1.37, loss_critic=44.4]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/ 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [07:35,  1.13s/it, gp=0.0235, loss_critic=5.4]   "
     ]
    }
   ],
   "source": [
    "gen = Generator(\n",
    "    Z_DIm, W_DIM, IN_CHANNELS, CHANNELS_IMG, \n",
    ").to(DEVICE)\n",
    "critic = Discriminator(IN_CHANNELS, CHANNELS_IMG).to(DEVICE)\n",
    "opt_gen = optim.Adam([{'params': [param for name, param in gen.named_parameters() if 'map' not in name]}],\n",
    "                     lr=LR, betas =(0.0, 0.99))\n",
    "opt_critic = optim.Adam(\n",
    "    critic.parameters(), lr= LR, betas =(0.0, 0.99)\n",
    ")\n",
    "\n",
    "gen.train()\n",
    "critic.train()\n",
    "step = int(log2(START_TRAIN_IMG_SIZE / 4))\n",
    "\n",
    "for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n",
    "    alpha = 1e-7\n",
    "    large_loader, large_dataset = get_loader(512, BATCH_SIZES[step])\n",
    "    loader, dataset = get_loader(4*2**step)\n",
    "    print('Curent image size: '+str(4*2**step))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch [{epoch + 1}/ {num_epochs}')\n",
    "        alpha = train_fn(\n",
    "            critic, gen, loader, dataset, large_loader, large_dataset, step, alpha, opt_critic, opt_gen\n",
    "        )\n",
    "        # if epoch % 10 == 0:\n",
    "        #     checkpoint = {\n",
    "        #     'state_dict_G': gen.state_dict(),\n",
    "        #     'state_dict_D': critic.state_dict(),\n",
    "        #     'opt_G': opt_gen.state_dict(),\n",
    "        #     'opt_D': opt_critic.state_dict(),\n",
    "        #     'size_batch': num_epochs,\n",
    "        #     'epoch': epoch,\n",
    "        #     'step': step+1\n",
    "        #     }\n",
    "        #     torch.save(checkpoint, \"checkpoint2.pth.tar\")\n",
    "\n",
    "    generate_examples(gen, step, large_loader)\n",
    "    if step > 4:\n",
    "        torch.save(gen.state_dict(), f'STYLEGAN_Discriminator{step}.pth')\n",
    "        torch.save(critic.state_dict(), f'STYLEGAN_Generator{step}.pth')\n",
    "    step +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = Generator(\n",
    "#     Z_DIm, W_DIM, IN_CHANNELS, CHANNELS_IMG\n",
    "# ).to(DEVICE)\n",
    "# critic = Discriminator(IN_CHANNELS, CHANNELS_IMG).to(DEVICE)\n",
    "# opt_gen = optim.Adam([{'params': [param for name, param in gen.named_parameters() if 'map' not in name]},\n",
    "#                      {'params': gen.map.parameters(), 'lr': 1e-5}], lr=LR, betas =(0.0, 0.99))\n",
    "# opt_critic = optim.Adam(\n",
    "#     critic.parameters(), lr= LR, betas =(0.0, 0.99)\n",
    "# )\n",
    "\n",
    "# checkpoint = torch.load(\"checkpoint.pth.tar\")\n",
    "# gen.load_state_dict(checkpoint['state_dict_G'])\n",
    "# critic.load_state_dict(checkpoint['state_dict_D'])\n",
    "# opt_gen.load_state_dict(checkpoint['opt_G'])\n",
    "# opt_critic.load_state_dict(checkpoint['opt_D'])\n",
    "# step = checkpoint['step'] - 1\n",
    "# print(step)\n",
    "# epoch = checkpoint['epoch']\n",
    "# size_batch = checkpoint['size_epochs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(\n",
    "    Z_DIm, W_DIM, IN_CHANNELS, CHANNELS_IMG\n",
    ").to(DEVICE)\n",
    "critic = Discriminator(IN_CHANNELS, CHANNELS_IMG).to(DEVICE)\n",
    "opt_gen = optim.Adam([{'params': [param for name, param in gen.named_parameters() if 'map' not in name]},\n",
    "                     {'params': gen.map.parameters(), 'lr': 1e-5}], lr=LR, betas =(0.0, 0.99))\n",
    "opt_critic = optim.Adam(\n",
    "    critic.parameters(), lr= LR, betas =(0.0, 0.99)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gen.state_dict(), 'STYLEGAN_Discriminator_labelled.pth')\n",
    "torch.save(critic.state_dict(), 'STYLEGAN_Generator_labelled.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic.load_state_dict(torch.load('STYLEGAN_Generator7.pth'))\n",
    "gen.load_state_dict(torch.load('STYLEGAN_Discriminator7.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_examples(gen, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0:\"AMD\", 1:\"DME\", 2:\"ERM\", 3:\"NO\", 4:\"RAO\", 5:\"RVO\", 6:\"VID\"}\n",
    "times = [1231, 147, 155, 332, 22, 101, 76]\n",
    "\n",
    "def generate_test_set(gen, steps):\n",
    "    c = 0\n",
    "    gen.eval()\n",
    "    alpha = 1.0\n",
    "    for i in range(len(times)):\n",
    "        for j in range(times[i]):\n",
    "            with torch.no_grad():\n",
    "                noise = torch.randn(1, Z_DIm).to(DEVICE)\n",
    "                label = torch.tensor([i]).to(DEVICE)\n",
    "                img = gen(noise, alpha, steps, label)\n",
    "                if not os.path.exists(f'test/{labels[i]}'):\n",
    "                    os.makedirs(f'test/{labels[i]}')\n",
    "                save_image(img*0.5+0.5, f\"test/{labels[i]}/img_{c}.png\")\n",
    "                c+=1\n",
    "    gen.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_test_set(gen, 7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "20682def01f3f467eb74e11ca1f7fb8aba52d6795e0e97b11e537403717040cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
