{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoiiKvhfaM6C"
      },
      "outputs": [],
      "source": [
        "# Load TensorBoard extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Start TensorBoard\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Set up a log directory for TensorBoard\n",
        "log_dir = os.path.join(\"logs\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "os.makedirs(log_dir, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PEKBoCpTvmnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import zscore\n",
        "import os\n",
        "import cv2\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "def preProcessing():\n",
        "    LABELS_PATH = '/content/drive/My Drive/OCTDL_labels.csv'\n",
        "    INPUT_PATH = '/content/drive/My Drive/OCTDL'\n",
        "    OUTPUT_PATH = '/content/drive/My Drive/augmented_data'\n",
        "\n",
        "    df = pd.read_csv(LABELS_PATH)\n",
        "\n",
        "    dimensions = pd.DataFrame(df, columns = ['file_name', 'image_width', 'image_hight'])\n",
        "\n",
        "    dimensions['z_width'] = zscore(dimensions['image_width'])\n",
        "    dimensions['z_height'] = zscore(dimensions['image_hight'])\n",
        "\n",
        "    z_threshold = 2.5\n",
        "\n",
        "    filtered_dimensions = dimensions[\n",
        "        (abs(dimensions['z_width']) <= z_threshold) &\n",
        "        (abs(dimensions['z_height']) <= z_threshold)\n",
        "    ].copy()\n",
        "\n",
        "\n",
        "    filtered_dimensions['aspect_ratio'] = filtered_dimensions['image_width'] / filtered_dimensions['image_hight']\n",
        "    avg_aspect_ratio = filtered_dimensions['aspect_ratio'].mean()\n",
        "\n",
        "    height = filtered_dimensions['image_hight'].min()\n",
        "    width = round(height * avg_aspect_ratio)\n",
        "    target_size = (width, height)\n",
        "\n",
        "    def resize_and_save(classname):\n",
        "        input_dir = INPUT_PATH + '/' + classname\n",
        "        output_dir = OUTPUT_PATH + '/' + classname\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok = True)\n",
        "\n",
        "        for _, row in filtered_dimensions.iterrows():\n",
        "                file_name = row['file_name'] + '.jpg'\n",
        "                if classname.lower() not in file_name:\n",
        "                    continue\n",
        "                image_path = os.path.join(input_dir, file_name)\n",
        "\n",
        "                # Read the image\n",
        "                img = cv2.imread(image_path)\n",
        "                if img is None:\n",
        "                    print(f\"Error loading image {file_name}\")\n",
        "                    continue\n",
        "\n",
        "                resized_img = cv2.resize(img, target_size, interpolation = cv2.INTER_AREA)\n",
        "\n",
        "                output_path = os.path.join(output_dir, file_name)\n",
        "                cv2.imwrite(output_path, resized_img)\n",
        "\n",
        "    classnames = ['AMD', 'DME', 'ERM', 'NO', 'RAO', 'RVO', 'VID']\n",
        "\n",
        "    #for classname in classnames:\n",
        "        #resize_and_save(classname)\n",
        "\n",
        "\n",
        "    # hyperparameters - to be adjusted\n",
        "    TEST_TRAIN_SPLIT = 0.3\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    dataset = datasets.ImageFolder(OUTPUT_PATH, transform=transform)\n",
        "\n",
        "    n_test = int(np.floor(TEST_TRAIN_SPLIT * len(dataset)))\n",
        "    n_train = len(dataset) - n_test\n",
        "\n",
        "    train_ds, test_ds = random_split(dataset, [n_train, n_test])\n",
        "\n",
        "    train_dl = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)\n",
        "    test_dl = DataLoader(test_ds, batch_size = BATCH_SIZE, shuffle = False, num_workers = 4)\n",
        "\n",
        "\n",
        "    # some useful info about the dataset\n",
        "    print(f\"Classes: {dataset.classes}\")\n",
        "    print(f\"Number of training samples: {len(train_ds)}\")\n",
        "    print(f\"Number of testing samples: {len(test_ds)}\")\n",
        "    #for i, (x, label) in enumerate(train_dl):\n",
        "        #print(label)\n",
        "        #break\n",
        "        #print(f\"Image shape: {x.shape}\")\n",
        "    return train_dl, test_dl"
      ],
      "metadata": {
        "id": "AMi_bCq0vaHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def gradient_penalty(critic, real, fake, device=\"cpu\"):\n",
        "    BATCH_SIZE, C, H, W = real.shape\n",
        "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "    interpolated_images = real * alpha + fake * (1 - alpha)\n",
        "\n",
        "    # Calculate critic scores\n",
        "    mixed_scores = critic(interpolated_images)\n",
        "\n",
        "    # Take the gradient of the scores with respect to the images\n",
        "    gradient = torch.autograd.grad(\n",
        "        inputs=interpolated_images,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
        "    return gradient_penalty"
      ],
      "metadata": {
        "id": "o2w70JS7mUKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels_img, features_d):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            #input image of size 546 * 199\n",
        "            nn.Conv2d(channels_img, features_d, kernel_size=[3,4], stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),# img: 273x100\n",
        "            self._block(features_d, features_d * 2, 4, 2, 1),  # img: 136x50\n",
        "            self._block(features_d * 2, features_d * 4, 4, 2, 1),  # img: 68x25\n",
        "            self._block(features_d * 4, features_d * 8, 4, 2, 1),  # img: 34x12\n",
        "            self._block(features_d * 8, features_d * 16, 4, 2, 1),  # img: 17x6\n",
        "            self._block(features_d * 16, features_d * 32, [3,4], [1,2], 1),  # img: 8x6\n",
        "            self._block(features_d * 32, features_d * 64, 4, 2, 1),  # img: 4x3\n",
        "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
        "            nn.Conv2d(features_d * 64, 1, kernel_size=[3,4], stride=[3,4], padding=0),\n",
        "        )\n",
        "\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.InstanceNorm2d(out_channels, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.disc(x)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # Input: N x channels_noise x 1 x 1\n",
        "            self._block(channels_noise, features_g * 128, [3,4], 1, 0),  # img: 4x3\n",
        "            self._block(features_g * 128, features_g * 64, [1,4], [1,2], [0,1]),  # img: 8x3\n",
        "            self._block(features_g * 64, features_g * 32, [4,5], 2, 1),  # img: 17x6\n",
        "            self._block(features_g * 32, features_g * 16, 4, 2, 1),  # img: 34x12\n",
        "            self._block(features_g * 16, features_g * 8, [5,4], 2, 1),  # img: 68x25\n",
        "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 136x50\n",
        "            self._block(features_g * 4, features_g * 2, [4,5], 2, 1),  # img: 273x100\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g * 2, channels_img, kernel_size=[3,4], stride=2, padding=1\n",
        "            ),\n",
        "            # Output: N x channels_img x 546 x 199\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def initialize_weights(model):\n",
        "    # Initializes weights according to the DCGAN paper\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "usX15zeEa0L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from google.colab import files\n",
        "\n",
        "# Hyperparameters etc.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 32\n",
        "CHANNELS_IMG = 1\n",
        "Z_DIM = 100\n",
        "NUM_EPOCHS = 200\n",
        "FEATURES_CRITIC = 16\n",
        "FEATURES_GEN = 16\n",
        "CRITIC_ITERATIONS = 5\n",
        "LAMBDA_GP = 10\n",
        "\n",
        "\n",
        "train_dl, test_dl = preProcessing()\n",
        "\n",
        "# initialize gen and disc, note: discriminator should be called critic,\n",
        "# according to WGAN paper (since it no longer outputs between [0, 1])\n",
        "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
        "critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(critic)\n",
        "\n",
        "# initializate optimizer\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
        "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
        "\n",
        "# for tensorboard plotting\n",
        "fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n",
        "writer_real = SummaryWriter(f\"logs/GAN_OCT/real\")\n",
        "writer_fake = SummaryWriter(f\"logs/GAN_OCT/fake\")\n",
        "%tensorboard --logdir logs --host 0.0.0.0 --port 6006\n",
        "step = 0\n",
        "\n",
        "gen.train()\n",
        "critic.train()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Target labels not needed! <3 unsupervised\n",
        "    for batch_idx, (real, _) in enumerate(train_dl):\n",
        "        real = real.to(device)\n",
        "        cur_batch_size = real.shape[0]\n",
        "        # Train Critic: max E[critic(real)] - E[critic(fake)]\n",
        "        # equivalent to minimizing the negative of that\n",
        "        for _ in range(CRITIC_ITERATIONS):\n",
        "            noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
        "            fake = gen(noise)\n",
        "            critic_real = critic(real).reshape(-1)\n",
        "            critic_fake = critic(fake).reshape(-1)\n",
        "            gp = gradient_penalty(critic, real, fake, device=device)\n",
        "            loss_critic = (\n",
        "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
        "            )\n",
        "            critic.zero_grad()\n",
        "            loss_critic.backward(retain_graph=True)\n",
        "            opt_critic.step()\n",
        "\n",
        "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
        "\n",
        "        gen_fake = critic(fake).reshape(-1)\n",
        "        loss_gen = -torch.mean(gen_fake)\n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Print losses occasionally and print to tensorboard\n",
        "        if batch_idx % 44 == 0 and batch_idx > 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx+1}/{len(train_dl)} \\\n",
        "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                fake = gen(fixed_noise)\n",
        "                # take out (up to) 32 examples\n",
        "                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
        "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
        "\n",
        "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
        "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
        "\n",
        "                output_path = \"fake_image.png\"\n",
        "                torchvision.utils.save_image(img_grid_fake, output_path)\n",
        "                files.download(output_path)\n",
        "\n",
        "            step += 1\n"
      ],
      "metadata": {
        "id": "MKZuXJ4Ca3Lg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}