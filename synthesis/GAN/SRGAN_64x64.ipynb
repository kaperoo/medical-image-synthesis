{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoiiKvhfaM6C"
      },
      "outputs": [],
      "source": [
        "# Load TensorBoard extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Start TensorBoard\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Set up a log directory for TensorBoard\n",
        "log_dir = os.path.join(\"logs\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# https://github.com/vishal1905/Super-Resolution/blob/master/SR_PT.ipynb\n",
        "# Has been the source of reference for much of the changes made to convert to a SRGAN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PEKBoCpTvmnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb38233e-a873-4c60-afb4-35be93e35bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INPORT DATA AND PREPROCESS"
      ],
      "metadata": {
        "id": "dcGgyMD3lPU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import zscore\n",
        "import os\n",
        "import cv2\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "import torchvision.utils\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "AMi_bCq0vaHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess images to lower resolution\n",
        "def preProcessing(OUTPUT_PATH):\n",
        "    LABELS_PATH = '/content/drive/My Drive/OCTDL_labels.csv'\n",
        "    INPUT_PATH = '/content/drive/My Drive/OCTDL_AUGMENTED'\n",
        "    #OUTPUT_PATH = '/content/drive/My Drive/OCTDL_DCGAN_AUGMENTED'\n",
        "\n",
        "    df = pd.read_csv(LABELS_PATH)\n",
        "\n",
        "    dimensions = pd.DataFrame(df, columns = ['file_name', 'image_width', 'image_hight'])\n",
        "\n",
        "    dimensions['z_width'] = zscore(dimensions['image_width'])\n",
        "    dimensions['z_height'] = zscore(dimensions['image_hight'])\n",
        "\n",
        "    z_threshold = 2.5\n",
        "\n",
        "    filtered_dimensions = dimensions[\n",
        "        (abs(dimensions['z_width']) <= z_threshold) &\n",
        "        (abs(dimensions['z_height']) <= z_threshold)\n",
        "    ].copy()\n",
        "\n",
        "\n",
        "    filtered_dimensions['aspect_ratio'] = filtered_dimensions['image_width'] / filtered_dimensions['image_hight']\n",
        "    avg_aspect_ratio = filtered_dimensions['aspect_ratio'].mean()\n",
        "\n",
        "    # We will try to make the SRgan work with 64x64 and scale up to the dimensions\n",
        "    # of other gans later\n",
        "    height = 64\n",
        "    width = 64\n",
        "    target_size = (width, height)\n",
        "\n",
        "    def resize_and_save(classname):\n",
        "        input_dir = INPUT_PATH + '/augmented_data/' + classname\n",
        "        output_dir = OUTPUT_PATH + '/' + classname\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok = True)\n",
        "\n",
        "        for _, row in filtered_dimensions.iterrows():\n",
        "                file_name = row['file_name'] + '.jpg'\n",
        "                if classname.lower() not in file_name:\n",
        "                    continue\n",
        "                image_path = os.path.join(input_dir, file_name)\n",
        "\n",
        "                # Read the image\n",
        "                img = cv2.imread(image_path)\n",
        "                if img is None:\n",
        "                    print(f\"Error loading image {file_name}\")\n",
        "                    continue\n",
        "\n",
        "                resized_img = cv2.resize(img, target_size, interpolation = cv2.INTER_AREA)\n",
        "\n",
        "                output_path = os.path.join(output_dir, file_name)\n",
        "                cv2.imwrite(output_path, resized_img)\n",
        "\n",
        "    classnames = ['AMD', 'DME', 'ERM', 'NO', 'RAO', 'RVO', 'VID']\n",
        "\n",
        "    #for classname in classnames:\n",
        "    #    resize_and_save(classname)\n",
        "\n",
        "\n",
        "    # hyperparameters - to be adjusted\n",
        "    # currently 70/30 split\n",
        "    TEST_TRAIN_SPLIT = 0.3\n",
        "    # we'll try 32 for now\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    num_channels = 1 # used later when declaring generator/descriminator\n",
        "    features_discrim = 16\n",
        "    # 64 by 64 images\n",
        "    dataset = datasets.ImageFolder(OUTPUT_PATH, transform=transform)\n",
        "\n",
        "    #full preprocessed images\n",
        "    #dataset = datasets.ImageFolder(INPUT_PATH, transform=transform)\n",
        "\n",
        "    n_test = int(np.floor(TEST_TRAIN_SPLIT * len(dataset)))\n",
        "    n_train = len(dataset) - n_test\n",
        "\n",
        "    train_ds, test_ds = random_split(dataset, [n_train, n_test])\n",
        "\n",
        "    train_dl = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)\n",
        "    test_dl = DataLoader(test_ds, batch_size = BATCH_SIZE, shuffle = False, num_workers = 4)\n",
        "\n",
        "\n",
        "    # some useful info about the dataset\n",
        "    print(f\"Classes: {dataset.classes}\")\n",
        "    print(f\"Number of training samples: {len(train_ds)}\")\n",
        "    print(f\"Number of testing samples: {len(test_ds)}\")\n",
        "    #for i, (x, label) in enumerate(train_dl):\n",
        "        #print(label)\n",
        "        #break\n",
        "        #print(f\"Image shape: {x.shape}\")\n",
        "    return train_dl, test_dl"
      ],
      "metadata": {
        "id": "J5V2y9Vodk5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Declaring Generator"
      ],
      "metadata": {
        "id": "sCiZIxrUlU2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Using https://github.com/vishal1905/Super-Resolution as a reference framework for the SRGAN\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3,64,9,padding=4,bias=False)\n",
        "        self.conv2 = nn.Conv2d(64,64,3,padding=1,bias=False)\n",
        "        self.conv3_1 = nn.Conv2d(64,256,3,padding=1,bias=False)\n",
        "        self.conv3_2 = nn.Conv2d(64,256,3,padding=1,bias=False)\n",
        "        self.conv4 = nn.Conv2d(64,3,9,padding=4,bias=False)\n",
        "        self.bn = nn.BatchNorm2d(64)\n",
        "        self.ps = nn.PixelShuffle(2)\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        block1 = self.prelu(self.conv1(input))\n",
        "        block2 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block1))))),block1)\n",
        "        block3 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block2))))),block2)\n",
        "        block4 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block3))))),block3)\n",
        "        block5 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block4))))),block4)\n",
        "        block6 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block5))))),block5)\n",
        "        block7 = torch.add(self.bn(self.conv2(block6)),block1)\n",
        "        block8 = self.prelu(self.ps(self.conv3_1(block7)))\n",
        "        block9 = self.prelu(self.ps(self.conv3_2(block8)))\n",
        "        block10 = self.conv4(block9)\n",
        "        return block10\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Create the generator\n",
        "netG = Generator().to(\"cuda\")\n",
        "\n",
        "# Handle multi-GPU if desired\n",
        "#if (device.type == 'cuda') and (ngpu > 1):\n",
        "#    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
        "\n",
        "\n",
        "# Print the model\n",
        "print(netG)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohYa4sW7j7ng",
        "outputId": "6d86ee90-849b-4a00-d5df-f2eb703abd47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), bias=False)\n",
            "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv3_1): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv3_2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv4): Conv2d(64, 3, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), bias=False)\n",
            "  (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (ps): PixelShuffle(upscale_factor=2)\n",
            "  (prelu): PReLU(num_parameters=1)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Declaring Discriminator"
      ],
      "metadata": {
        "id": "etlX-sWXlZCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using https://github.com/vishal1905/Super-Resolution as a reference framework for the SRGAN\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3,64,3,padding=1,bias=False)\n",
        "        self.conv2 = nn.Conv2d(64,64,3,stride=2,padding=1,bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64,128,3,padding=1,bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128,128,3,stride=2,padding=1,bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.conv5 = nn.Conv2d(128,256,3,padding=1,bias=False)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.conv6 = nn.Conv2d(256,256,3,stride=2,padding=1,bias=False)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        self.conv7 = nn.Conv2d(256,512,3,padding=1,bias=False)\n",
        "        self.bn7 = nn.BatchNorm2d(512)\n",
        "        self.conv8 = nn.Conv2d(512,512,3,stride=2,padding=1,bias=False)\n",
        "        self.bn8 = nn.BatchNorm2d(512)\n",
        "        self.fc1 = nn.Linear(512*16*16,1024)\n",
        "        self.fc2 = nn.Linear(1024,1)\n",
        "        self.drop = nn.Dropout2d(0.3)\n",
        "\n",
        "    def forward(self,input):\n",
        "        block1 = nn.functional.leaky_relu(self.conv1(input))\n",
        "        block2 = nn.functional.leaky_relu(self.bn2(self.conv2(block1)))\n",
        "        block3 = nn.functional.leaky_relu(self.bn3(self.conv3(block2)))\n",
        "        block4 = nn.functional.leaky_relu(self.bn4(self.conv4(block3)))\n",
        "        block5 = nn.functional.leaky_relu(self.bn5(self.conv5(block4)))\n",
        "        block6 = nn.functional.leaky_relu(self.bn6(self.conv6(block5)))\n",
        "        block7 = nn.functional.leaky_relu(self.bn7(self.conv7(block6)))\n",
        "        block8 = nn.functional.leaky_relu(self.bn8(self.conv8(block7)))\n",
        "        block8 = block8.view(-1,block8.size(1)*block8.size(2)*block8.size(3))\n",
        "        block9 = nn.functional.leaky_relu(self.fc1(block8),)\n",
        "#         block9 = block9.view(-1,block9.size(1)*block9.size(2)*block9.size(3))\n",
        "        block10 = torch.sigmoid(self.drop(self.fc2(block9)))\n",
        "        return block9,block10\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Create the Discriminator\n",
        "netD = Discriminator().to(\"cuda\")\n",
        "\n",
        "# Handle multi-GPU if desired\n",
        "#if (device.type == 'cuda') and (ngpu > 1):\n",
        "#    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
        "\n",
        "# Print the model\n",
        "print(netD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4x2VwSEkAqY",
        "outputId": "75080927-b4e6-448e-9121-932b42a66903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminator(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (bn6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv8): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (bn8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=131072, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (drop): Dropout2d(p=0.3, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#initialise loss function and optimisers"
      ],
      "metadata": {
        "id": "XFHDs-jLlcJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "# Initialize 4 loss functions. SRGANs use a variety of loss functions.\n",
        "gen_loss = nn.BCELoss()\n",
        "vgg_loss = nn.MSELoss()\n",
        "mse_loss = nn.MSELoss()\n",
        "disc_loss = nn.BCELoss()\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "fixed_noise = torch.randn(64, 100, 1, 1, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "LEARNING_RATE = 0.0002\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))"
      ],
      "metadata": {
        "id": "N4wJwUEUkrfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Begin Training"
      ],
      "metadata": {
        "id": "jsl1Hrsklsei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "from tqdm import tqdm\n",
        "# Using https://github.com/vishal1905/Super-Resolution as a reference for the implementation of loss functions\n",
        "# Lists to keep track of progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "num_epochs = 50\n",
        "train_lr, test_lr, = preProcessing('/content/drive/My Drive/OCTDL_DCGAN_AUGMENTED') # 64x64 images\n",
        "train_hr, test_hr, = preProcessing('/content/drive/My Drive/OCTDL_SRGAN_AUGMENTED') # 256x256 images\n",
        "print(\"Starting Training Loop...\")\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # For each batch in the dataloader\n",
        "    d1loss_list=[]\n",
        "    d2loss_list=[]\n",
        "    gloss_list=[]\n",
        "    vloss_list=[]\n",
        "    mloss_list=[]\n",
        "    for batch in tqdm(range(1)):\n",
        "        lr_images = np.stack(train_lr)\n",
        "        hr_images = np.stack(train_hr)\n",
        "\n",
        "\n",
        "        netD.zero_grad()\n",
        "\n",
        "        gen_out = netG(torch.from_numpy(lr_images).to(\"cuda\").float())\n",
        "        _,f_label = netD(gen_out)\n",
        "        _,r_label = netD(torch.from_numpy(hr_images).to(\"cuda\").float())\n",
        "        d1_loss = (disc_loss(f_label,torch.zeros_like(f_label,dtype=torch.float)))\n",
        "        d2_loss = (disc_loss(r_label,torch.ones_like(r_label,dtype=torch.float)))\n",
        "        # d_loss = d1_loss+d2_loss\n",
        "        d2_loss.backward()\n",
        "        d1_loss.backward(retain_graph=True)\n",
        "        # print(d1_loss,d2_loss)\n",
        "#         d_loss.backward(retain_graph=True)\n",
        "        optimizerD.step()\n",
        "\n",
        "\n",
        "        netG.zero_grad()\n",
        "        g_loss = gen_loss(f_label.data,torch.ones_like(f_label,dtype=torch.float))\n",
        "        m_loss = mse_loss(gen_out,torch.from_numpy(hr_images).to(\"cuda\").float())\n",
        "\n",
        "        generator_loss = g_loss + m_loss\n",
        "        # v_loss.backward(retain_graph=True)\n",
        "        # m_loss.backward(retain_graph=True)\n",
        "        # g_loss.backward()\n",
        "        # print(generator_loss)\n",
        "\n",
        "        generator_loss.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "        d1loss_list.append(d1_loss.item())\n",
        "        d2loss_list.append(d2_loss.item())\n",
        "\n",
        "        gloss_list.append(g_loss.item())\n",
        "        mloss_list.append(m_loss.item())\n",
        "\n",
        "\n",
        "\n",
        "#         print(\"d1Loss ::: \"+str((d1_loss.item()))+\" d2Loss ::: \"+str((d2_loss.item())))\n",
        "#         print(\"gloss ::: \"+str((g_loss.item()))+\" vloss ::: \"+str((v_loss.item()))+\" mloss ::: \"+str((m_loss.item())))\n",
        "    print(\"Epoch ::::  \"+str(epoch+1)+\"  d1_loss ::: \"+str(np.mean(d1loss_list))+\"  d2_loss :::\"+str(np.mean(d2loss_list)))\n",
        "    print(\"genLoss ::: \"+str(np.mean(gloss_list))+\"  vggLoss ::: \"+str(np.mean(vloss_list))+\"  MeanLoss  ::: \"+str(np.mean(mloss_list)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "lxf_i97nlGME",
        "outputId": "1c2d6f02-d7b0-4e2b-8383-8726d33700fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training Loop...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.20 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.02 GiB is free. Process 2530 has 13.72 GiB memory in use. Of the allocated memory 12.55 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-187c4ed681dd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mnetD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mgen_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhr_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-b90e37ee25c9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mblock1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mblock2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mblock1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mblock3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mblock2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mblock4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mblock3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mblock5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mblock4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.20 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.02 GiB is free. Process 2530 has 13.72 GiB memory in use. Of the allocated memory 12.55 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_lr)"
      ],
      "metadata": {
        "id": "HVN476W4pIum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plot Results"
      ],
      "metadata": {
        "id": "58QRwSQ3lwYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "save the models"
      ],
      "metadata": {
        "id": "fR-9ogXWhp5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(netG.state_dict, '/content/drive/My Drive/OCTDL_DCGAN_AUGMENTED/netGSR.pth')\n",
        "torch.save(netD.state_dict, '/content/drive/My Drive/OCTDL_DCGAN_AUGMENTED/netDSR.pth')\n",
        "torch.save(netG, '/content/drive/My Drive/OCTDL_DCGAN_AUGMENTED/netGSRfull.pth')\n",
        "torch.save(netD, '/content/drive/My Drive/OCTDL_DCGAN_AUGMENTED/netDSRfull.pth')\n",
        "netG.eval()\n",
        "netD.eval()"
      ],
      "metadata": {
        "id": "Z86WAFkqhUoD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}